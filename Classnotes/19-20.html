<!DOCTYPE html>
<html >
<head>
  <meta charset="UTF-8">
  <title>Chapter 3</title>
  
  
  
      <link rel="stylesheet" href="css/style.css">

  
</head>

<body>
  <div id="wrapper">
    <div id="container">

        <section class="open-book">
            <header>
                <h1><a href="17-18.html">Previous</a></h1>
				<h1><a href="21-22.html">Next</a></h1>
            </header>
            <article>
						-----------------------------------------------------------------------------------------

					<h2>Big data</h2>
					Big Data is a collection of data that is huge in volume, yet growing exponentially with time. It is data with so large size and complexity that none of the traditional data management tools can store it or process it efficiently.

					<h3>Examples Of Big Data</h3>
					<li>The New York Stock Exchange is an example of Big Data that generates about one terabyte of new trade data per day.</li>
					<li>Social Media:  The statistic shows that 500+terabytes of new data get ingested into the databases of social media site Facebook, every day. This data is mainly generated in terms of photo and video uploads, message exchanges, putting comments, etc.</li>
					<li>A single Jet engine can generate 10+terabytes of data in 30 minutes of flight time. With many thousand flights per day, generation of data reaches up to many Petabytes.</li>

					<br><h3>Types Of Big Data</h3>
					<b>Following are the types of Big Data:</b>
					<li>Structured</li>
					<li>Unstructured</li>
					<li>Semi-structured</li>

					<br><h3>Structured</h3>
					Any data that can be stored, accessed, and processed in the form of fixed-format is termed as ‘structured’ data. Over the period of time, ways in computer science has achieved greater success in developing techniques for working with such kind of data (where the format is well known in advance) and also deriving value out of it. However, nowadays, we are foreseeing issues when the size of such data grows to a huge extent, typical sizes are being in the rage of multiple zettabytes.

					<br><b><em>10^21 bytes equal to 1 zettabyte or one billion terabytes form a zettabyte.</em></b>

					<br>Looking at these figures one can easily understand why the name Big Data is given and imagine the challenges involved in its storage and processing.
					<br>Data stored in a relational database management system is one example of a ‘structured’ data.

					<br><b>Examples Of Structured Data</b>
					<br>An ‘Employee’ table in a database is an example of Structured Data
					<br><img src="images/47.png">
					
					<br><h3>Unstructured</h3>
					Any data with an unknown form or structure is classified as unstructured data. In addition to the size being huge, unstructured data poses multiple challenges in terms of its processing for deriving value out of it. A typical example of unstructured data is a heterogeneous data source containing a combination of simple text files, images, videos, etc. Now day organizations have wealth of data available with them but unfortunately, they don’t know how to derive value out of it since this data is in its raw form or unstructured format.
					<br><b>Examples Of Unstructured Data</b>
					<br>The output returned by ‘Google Search’
					<br><img src="images/48.png">
					
					<br><h3>Semi-structured</h3>
					Semi-structured data can contain both forms of data. We can see semi-structured data as structured in form but it is actually not defined with e.g. a table definition in relational DBMS. An example of semi-structured data is data represented in an XML file.

					<br><b>Data Growth over the years</b>
					<br><img src="images/49.png">
					
					<h3>Characteristics Of Big Data</h3>
						<br><b>Big data can be described by the following characteristics:</b>
						<li>Volume</li>
						<li>Variety</li>
						<li>Velocity</li>
						<li>Variability</li>
						<li>Big Data is emerging as an opportunity for organizations.</li>
						<li>sets to uncover all hidden patterns, unknown correlations, market trends, customer preferences, and other useful business information.</li>
                        <li><em><a href="https://www.youtube.com/watch?v=aReuLtY0YMI">Click to watch detailed tutorial on Big Data</a></em></li>


					<br><br><h2>Hadoop Framework</h2>
						<li>Hadoop is an Apache open-source framework written in java that allows to storage and process of big data in a distributed environment across clusters of computers using simple programming models. It is designed to scale up from single servers to thousands of machines, each offering local computation and storage.</li>
						<li>Hadoop runs applications using the MapReduce algorithm, where the data is processed in parallel with others. In short, Hadoop is used to develop applications that could perform complete statistical analysis on huge amounts of data.</li>
						<li>Hadoop is not a type of database, but rather a software ecosystem that allows for massively parallel computing. It is an enabler of certain types of NoSQL distributed databases (such as HBase), which can allow for data to be spread across thousands of servers with little reduction in performance.</li>
                     <br><img src="images/50.png">
					
					<br><b>At its core, Hadoop has two major layers namely −</b>

					<br>1. Processing/Computation layer (MapReduce)
					<br>2. Storage layer (Hadoop Distributed File System).

					<br>MapReduce is a programming model or pattern within the Hadoop framework that is used to access big data stored in the Hadoop File System (HDFS). It is a core component, integral to the functioning of the Hadoop framework.

					<br>Suppose  we have a library that has a collection of a huge number of books on each floor
					<li>First case: if we will do it by ourself.</li>
					<li>Second, ask three friends to do this task. Each friend will count the books for 1 particular floor. [Faster and easy to do] [This is similar to parallel processing] [Each machine will have its own memory and processing power].</li>
					<li>Time consumption will be less and in the end, we can combine the result.</li>
					<li>Each person will map the data of each floor (activity).</li>
					<li>Mapreduce process the data on different node machines.</li>
					<li>Mapper class maps the data present in different data nodes.</li>
					<li>A reducer class aggregates and reduces the output of different data nodes to generate the final output.</li>

					<h3>HDFS Architecture</h3>

					<li>Apache HDFS or Hadoop Distributed File System is a block-structured file system where each file is divided into blocks of a pre-determined size.</li>
					<li>These blocks are stored across a cluster of one or several machines.</li>
					<li>Apache Hadoop HDFS Architecture follows a Master/Slave Architecture, where a cluster comprises of a single NameNode (Master node) and all the other nodes are DataNodes (Slave nodes).</li>
					<li>HDFS can be deployed on a broad spectrum of machines that support Java.</li>


					<br>Hardware failure is the norm rather than the exception. An HDFS instance may consist of hundreds or thousands of server machines, each storing part of the file system’s data. The fact that there are a huge number of components and that each component has a non-trivial probability of failure means that some component of HDFS is always non-functional. Therefore, detection of faults and quick, automatic recovery from them is a core architectural goal of HDFS.

					<li>Internally, a file is split into one or more blocks and these blocks are stored in a set of DataNodes.</li>
					<li>The NameNode executes file system namespace operations like opening, closing, and renaming files and directories.</li>
					<li>It also determines the mapping of blocks to DataNodes.</li>
					<li>The DataNodes are responsible for serving read and write requests from the file system’s clients.</li>
					<li>The DataNodes also perform block creation, deletion, and replication upon instruction from the NameNode.</li>
				
				    <br><img src="images/51.png">
				
				<li>NameNode is the master node in the Apache Hadoop HDFS Architecture that maintains and manages the blocks present on the DataNodes (slave nodes).</li>
				<li>NameNode is a very highly available server that manages the File System Namespace and controls access to files by clients.</li>
				<li>The user data never resides on the NameNode. The data resides on DataNodes only.</li>
		
         		<br><h3>Functions of NameNode:</h3>
					<li>It is the master daemon that maintains and manages the DataNodes (slave nodes).</li>
					<p><li>It records the metadata of all the files stored in the cluster, e.g. The location of blocks stored, the size of the files, permissions, hierarchy, etc. There are two files associated with the metadata:
					<br>1. FsImage: It contains the complete state of the file system namespace since the start of the NameNode.
					<br>2. EditLogs: It contains all the recent modifications made to the file system with respect to the most recent FsImage.</p>
					<li>It records each change that takes place to the file system metadata. For example, if a file is deleted in HDFS, the NameNode will immediately record this in the EditLog.</li>
					<li>It regularly receives a Heartbeat and a block report from all the DataNodes in the cluster to ensure that the DataNodes are live.</li>
					<li>It keeps a record of all the blocks in HDFS and in which nodes these blocks are located.</li>
					<li>The NameNode is also responsible to take care of the replication factor of all the blocks.</li>
					<li>In case of the DataNode failure, the NameNode chooses new DataNodes for new replicas, balances disk usage, and manages the communication traffic to the DataNodes.</li>
					 

					<br><h3>DataNode:</h3>
					DataNodes are the slave nodes in HDFS. Unlike NameNode, DataNode is a non-expensive system that is not of high quality or high availability. The DataNode is a block server that stores the data in the local file ext3 or ext4.

					<br><em>Functions of DataNode:</em>
					<li>These are slave daemons or process which runs on each slave machine.</li>
					<li>The actual data is stored on DataNodes.</li>
					<li>The DataNodes perform the low-level read and write requests from the file system’s clients.</li>
					<li>They send heartbeats to the NameNode periodically to report the overall health of HDFS, by default, this frequency is set to 3 seconds.</li>
					<br>Till now, you must have realized that the NameNode is pretty much important to us. If it fails, we are doomed.  But don’t worry, we will be talking about how Hadoop solved this single point of failure problem in the next Apache Hadoop HDFS Architecture blog. So, just relax for now and let’s take one step at a time.

					<br><br><h3>Secondary NameNode:</h3>
					Apart from these two daemons, there is a third daemon or a process called Secondary NameNode. The Secondary NameNode works concurrently with the primary NameNode as a helper daemon. And don’t be confused about the Secondary NameNode being a backup NameNode because it is not.
					<br><em>Functions of Secondary NameNode:</em>
					<li>The Secondary NameNode is one that constantly reads all the file systems and metadata from the RAM of the NameNode and writes it into the hard disk or the file system.</li>
					<li>It is responsible for combining the EditLogs with FsImage from the NameNode. </li>
					<li>It downloads the EditLogs from the NameNode at regular intervals and applies to FsImage. The new FsImage is copied back to the NameNode, which is used whenever the NameNode is started the next time.</li>
					<br>Hence, Secondary NameNode performs regular checkpoints in HDFS. Therefore, it is also called CheckpointNode.

					<br><img src="images/52.png">

					<li><em><a href="https://www.youtube.com/watch?v=GJYEsEEfjvk&t=520s">Click to watch detailed tutorial on Big Data</a></em></li>

					<br><br><h3>Map Reduce</h3>
					MapReduce is a software framework and programming model used for processing huge amounts of data. MapReduce program work in two phases, namely, Map and Reduce. Map tasks deal with splitting and mapping of data while Reduce tasks shuffle and reduce the data.

					<br>Hadoop is capable of running MapReduce programs written in various languages: Java, Ruby, Python, and C++.
					<br>The programs of Map Reduce in cloud computing are parallel in nature, thus are very useful for performing large-scale data analysis using multiple machines in the cluster.
					<br>The whole process goes through four phases of execution namely, splitting, mapping, shuffling, and reducing.

					<br><img src="images/53.png">

					<h3>How MapReduce Organizes Work?</h3>
					<br>Hadoop divides the job into tasks. There are two types of tasks:
					<br>1. Map tasks (Splits & Mapping)
					<br>2. Reduce tasks (Shuffling, Reducing)

					<br>As mentioned above.
					<br>The complete execution process (execution of Map and Reduce tasks, both) is controlled by two types of entities called a
					<br>1. Jobtracker: Acts like a master (responsible for complete execution of submitted job)
					<br>2. Multiple Task Trackers: Acts like slaves, each of them performing the job
					<br>For every job submitted for execution in the system, there is one Jobtracker that resides on Namenode and there are multiple task trackers which reside on Datanode.

					<li>A job is divided into multiple tasks which are then run onto multiple data nodes in a cluster.
					<li>It is the responsibility of the job tracker to coordinate the activity by scheduling tasks to run on different data nodes.
					<li>Execution of individual tasks is then to look after by task tracker, which resides on every data node executing part of the job.
					<li>The task tracker’s responsibility is to send the progress report to the job tracker.
					<li>In addition, the task tracker periodically sends a ‘heartbeat’ signal to the Jobtracker so as to notify him of the current state of the system.
					<li>Thus job tracker keeps track of the overall progress of each job. In the event of task failure, the job tracker can reschedule it on a different task tracker.
					<br><img src="images/54.png">

<h2>YARN</h2>
YARN stands for “Yet Another Resource Negotiator“. It was introduced in Hadoop 2.0 to remove the bottleneck on Job Tracker which was present in Hadoop 1.0. YARN was described as a “Redesigned Resource Manager” at the time of its launching, but it has now evolved to be known as large-scale distributed operating system used for Big Data processing.
<br>YARN allows the data stored in HDFS (Hadoop Distributed File System) to be processed and run by various data processing engines such as batch processing, stream processing, interactive processing, graph processing, and many more. Thus the efficiency of the system is increased with the use of YARN.
					
			</article>
            <footer>
                <ol id="page-numbers">
                    <li>19</li>
                    <li>20</li>
                </ol>
            </footer>
        </section>

    </div>
</div>
  
  
</body>
</html>
